{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laragazzadelsole/AMD-Exam/blob/main/Amazon_US_costumer_Review_Link_Analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFbmuvIaVgp6"
      },
      "source": [
        "# Project description\n",
        "\n",
        "The aim of this project is to create a ranking system starting from the\n",
        "that could be used in order to establish the sequence in which the reviews should be shown. The\n",
        "idea is to calculate the PageRank score of the customers based on the reviews they did of an item\n",
        "in common with other customers. This is because it’s difficult that spammers (that want to inflate\n",
        "the rating of their products) have reviews in common with others of different products. However, in\n",
        "this way it would be given more importance to customers that left many, but meaningless reviews\n",
        "compared to those who left fewer, but more effective reviews. The PageRank score doesn’t give\n",
        "any information of the usefulness of the review left by users. However, on Amazon it is possible\n",
        "for users to rate a review as ”useful”. This data is captured by the variable helpf ul votes. So, in\n",
        "addition to the PageRank score the ”helpfulness score” is evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6AXvunXexkS"
      },
      "source": [
        " <font size=\"5\">**1. Initial Setup**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP1ksIVLelsr"
      },
      "source": [
        "**1.1 Data Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4UuM6knVsVy",
        "outputId": "7da77a8b-1456-4ed2-e63c-6bfc364a4bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Downloading amazon-us-customer-reviews-dataset.zip to /content\n",
            "100% 20.9G/21.0G [03:13<00:00, 94.2MB/s]\n",
            "100% 21.0G/21.0G [03:13<00:00, 116MB/s] \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"saragironi\"\n",
        "os.environ['KAGGLE_KEY'] = \"4b28e3c84038475619b3fff13d413869\"\n",
        "!pip install kaggle --upgrade\n",
        "!kaggle datasets download -d cynthiarempel/amazon-us-customer-reviews-dataset --unzip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5caK9hshfiSi"
      },
      "source": [
        "**1.2 Initializing Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8LMxdVJJ4iN",
        "outputId": "ea4ab1ce-29f3-4d16-dce8-88f2ca7421c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.18\" 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "#installing Java 8\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9EEJSFWiT-Q6"
      },
      "outputs": [],
      "source": [
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXYoAQbUUKvv",
        "outputId": "4e292139-618d-43bc-b94e-deb81ac68e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=e27f22c8b9be87d3a3c2e5c17d0b3aec3881be11defc49f38c7a45cfd5a9a3b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "# install findspark and pyspark using pip\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4lQmk7PvjjY2"
      },
      "outputs": [],
      "source": [
        "#import findspark \n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "lV0rAARMkD5n",
        "outputId": "8b5bd350-f4c1-4d18-8768-4b5abfa14c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f24f6ea22e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://348e7848eb6c:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#create a SparkSession\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvDJmxfQMOkX"
      },
      "source": [
        "<font size=\"5\">**2. The Data**</font> \\\n",
        "\n",
        "I have decided to focus the link analysis on the customers that left reviews on electronical products. The reason for this choice is that this kind of reviews tend to be more technical and less subjective due to the nature of the product, compared to other categories (such as Books, Music, Grocery, etc). \n",
        "Therefore, there could be a higher number of \"helpful_votes\", which is an important element for my purpose. The chosen categories are: Electronics and PC. These two categories are correlated as people purchasing a pc on Amazon could also probably buy a mouse or other electronical items. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_9tVo46LexoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182f0ba9-8866-497e-8ca2-e5844123306e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|         US|   41409413|R2MTG1GCZLR2DK|B00428R89M|     112201306|yoomall 5M Antenn...|     Electronics|          5|            0|          0|   N|                Y|          Five Stars|       As described.| 2015-08-31|\n",
            "|         US|   49668221|R2HBOEM8LE9928|B000068O48|     734576678|Hosa GPM-103 3.5m...|     Electronics|          5|            0|          0|   N|                Y|It works as adver...|It works as adver...| 2015-08-31|\n",
            "|         US|   12338275|R1P4RW1R9FDPEE|B000GGKOG8|     614448099|Channel Master Ti...|     Electronics|          5|            1|          1|   N|                Y|          Five Stars|         Works pissa| 2015-08-31|\n",
            "|         US|   38487968|R1EBPM82ENI67M|B000NU4OTA|      72265257|LIMTECH Wall char...|     Electronics|          1|            0|          0|   N|                Y|            One Star|Did not work at all.| 2015-08-31|\n",
            "|         US|   23732619|R372S58V6D11AT|B00JOQIO6S|     308169188|Skullcandy Air Ra...|     Electronics|          5|            1|          1|   N|                Y|Overall pleased w...|Works well. Bass ...| 2015-08-31|\n",
            "|         US|   21257820|R1A4514XOYI1PD|B008NCD2LG|     976385982|Pioneer SP-BS22-L...|     Electronics|          5|            1|          1|   N|                Y|          Five Stars|The quality on th...| 2015-08-31|\n",
            "|         US|    3084991|R20D9EHB7N20V6|B00007FGUF|     670878953|C2G/Cables to Go ...|     Electronics|          5|            0|          0|   N|                Y|           Lifesaver|Wish I could give...| 2015-08-31|\n",
            "|         US|    8153674|R1WUTD8MVSROJU|B00M9V2RMM|     508452933|COOLEAD-HDMI Swit...|     Electronics|          5|            0|          0|   N|                Y|          Five Stars|         works great| 2015-08-31|\n",
            "|         US|   52246189|R1QCYLT25812DM|B00J3O9DYI|     766372886|Philips Wireless ...|     Electronics|          4|            0|          0|   N|                Y|          Four Stars|Great sound and c...| 2015-08-31|\n",
            "|         US|   41463864| R904DQPBCEM7A|B00NS1A0E4|     458130381|PlayStation 3 3D ...|     Electronics|          4|            0|          0|   N|                Y|          Four Stars|    It works well~~~| 2015-08-31|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|\n",
            "|         US|   30088427| RQ28TSA020Y6J|B013ALA9LA|     671157305|TP-Link OnHub AC1...|              PC|          5|           24|         31|   N|                N|OnHub is a pretty...|I am a Google emp...| 2015-08-31|\n",
            "|         US|   20329786| RUXJRZCT6953M|B00PML2GQ8|     982036237|AmazonBasics USB ...|              PC|          1|            2|          2|   N|                N|None of them work...|Bought cables in ...| 2015-08-31|\n",
            "|         US|   14215710| R7EO0UO6BPB71|B001NS0OZ4|     576587596|Transcend P8 15-i...|              PC|          1|            0|          0|   N|                Y|just keep searching.|nope, cheap and slow| 2015-08-31|\n",
            "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|\n",
            "|         US|   30548466|R31SR7REWNX7CF|B00KX4TORI|     170101802|Kingston Digital ...|              PC|          5|            0|          0|   N|                Y|Good quality, wor...|Good quality,work...| 2015-08-31|\n",
            "|         US|     589298| RVBP8I1R0CTZ8|B00P17WEMY|     206124740|White 9 Inch Unlo...|              PC|          3|            1|          2|   N|                Y|in fact this is t...|This demn tablet ...| 2015-08-31|\n",
            "|         US|   49329488|R1QF6RS1PDLU18|B00TR05L9Y|     778403103|Lenovo TAB2 A10 -...|              PC|          4|            1|          1|   N|                Y|                Good|I am not sure I d...| 2015-08-31|\n",
            "|         US|   50728290|R23AICGEDAJQL1|B0098Y77OG|     177098042|                Acer|              PC|          1|            0|          0|   N|                Y|You get what you ...|After exactly 45 ...| 2015-08-31|\n",
            "|         US|   37802374|R2EY3N4K9W19UP|B00IFYEYXC|     602496520|AzureWave Broadco...|              PC|          5|            3|          4|   N|                Y|Great for Windows...|Replaced my Intel...| 2015-08-31|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The dataset is divided per categories. Here I define only those in which I am interested in. \n",
        "\n",
        "df_el = spark.read.csv('amazon_reviews_us_Electronics_v1_00.tsv', sep='\\t', header=True)\n",
        "df_pc = spark.read.csv('amazon_reviews_us_PC_v1_00.tsv', sep='\\t', header=True)\n",
        "df_el.show(10)\n",
        "df_pc.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs8u2ApAz8AR",
        "outputId": "0de5dff1-7a2f-4e21-8842-102c76c9378f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df = df_pc.union(df_el)\n",
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyHfo7ND6J41",
        "outputId": "e664a81f-fcb5-4916-a13a-c461650139d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: string (nullable = true)\n",
            " |-- helpful_votes: string (nullable = true)\n",
            " |-- total_votes: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Drop the columns that are not really useful for the purpose of this project\n",
        "\n",
        "df = df.drop(\"marketplace\",\"vine\", \"product_parent\", \"verfied_purchase\", \"review_headline\", \"review_body\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYhyVb9va8oD",
        "outputId": "5314650d-15c8-44d5-da2d-1b5b3dce8a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows: 10002423\n",
            "CPU times: user 205 ms, sys: 31.8 ms, total: 237 ms\n",
            "Wall time: 42.4 s\n"
          ]
        }
      ],
      "source": [
        "#The dataframe contains more than 10 millions rows. Here I want to compare the time difference between the action .count() and spark.sql commands.\n",
        "# .count() will be used further as it's faster\n",
        "\n",
        "%%time \n",
        "tot_rows = df.count()\n",
        "print(f\"Total number of rows: {tot_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAanG4r9zXRO",
        "outputId": "be127d45-7af3-4e7d-9387-aa3204e1dd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|total_rows|\n",
            "+----------+\n",
            "|  10002423|\n",
            "+----------+\n",
            "\n",
            "CPU times: user 257 ms, sys: 30.9 ms, total: 288 ms\n",
            "Wall time: 52.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df.createOrReplaceTempView(\"df_view\")\n",
        "total_rows = spark.sql(\"\"\"SELECT COUNT(review_id) AS total_rows \n",
        "                        FROM df_view\"\"\")\n",
        "total_rows.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Spark is lazy the action count() is faster. For this reason I'll use this method instead of spark.sql() in the rest of the code"
      ],
      "metadata": {
        "id": "DrVEvCB7BxfL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wj_TmvmmNmP"
      },
      "source": [
        "**2.1 Data Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnesUNdnmm0B",
        "outputId": "be6b60dd-9d8a-4c6c-c1fc-89531eebdd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+-----------------+-----------+\n",
            "|customer_id|review_id|product_id|product_title|product_category|star_rating|helpful_votes|total_votes|verified_purchase|review_date|\n",
            "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+-----------------+-----------+\n",
            "|          0|        0|         0|            0|              11|         11|           11|         11|               11|        333|\n",
            "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+-----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if there are null values \n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Du4IFhHodd8",
        "outputId": "d09402f9-d09c-4274-e2e0-fa4fcc674a9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10002090"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Rows with missing data are deleted\n",
        "\n",
        "df_complete = df.na.drop()\n",
        "df_complete.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbNZvQ-VyQbi",
        "outputId": "d0d07334-3222-4c52-fc87-26905d635cfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# drop duplicates of reviews if any\n",
        "\n",
        "df_final = df_complete.dropDuplicates([\"review_id\"])\n",
        "type(df_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SfyxJqGFjRE",
        "outputId": "44d59e7c-8752-48d0-c697-6ff75ab5f35b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#check for anomalies in the values like star_rating > 5\n",
        "#first cast the data type of \"star_rating\" from string to int\n",
        "\n",
        "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
        "from pyspark.sql.functions import col,when,count\n",
        "\n",
        "df_final = df_final.withColumn(\"star_rating\",df_final.star_rating.cast('int'))\n",
        "\n",
        "df_final.select('review_id').where(df_final.star_rating>5).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgJgwh9XnJ0H",
        "outputId": "7076c8dd-7ffd-440b-e9e7-6848f53adb57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#check for consistencies: if \"helpful_votes\" is always lower or equal to \"total_votes\"\n",
        "\n",
        "df_final = df_final.withColumn(\"helpful_votes\",df_final.helpful_votes.cast('int'))\n",
        "df_final = df_final.withColumn(\"total_votes\",df_final.total_votes.cast('int'))\n",
        "\n",
        "df_final.select('review_id').where(df_final.helpful_votes>df_final.total_votes).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAVlShE7yk95"
      },
      "source": [
        "The dataframe is now complete and there aren't inconsistencies. However, if I were to implement the algorithm on the dataset as a whole it would be too computationally intense. For this reason a subsample of 50000 rows is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZBE8gzcPi9UL"
      },
      "outputs": [],
      "source": [
        "sub_df = df_final.limit(50000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n90Qn9l8xQk"
      },
      "source": [
        "<font size=\"5\">**3. Exploratory Data Analysis**</font> \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8bfEqV7aFAJ",
        "outputId": "f9c841ee-a46e-4bd9-99d4-acffe994a3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average number of reviews per person is: 1.0109792344865236\n"
          ]
        }
      ],
      "source": [
        "#calculate the average number of reviews per person\n",
        "\n",
        "from pyspark.sql.functions import count, desc, avg\n",
        "\n",
        "# Group the data by person and count the number of reviews per person\n",
        "reviews_per_person = sub_df.groupby(\"customer_id\").count().select(\"count\")\n",
        "\n",
        "# Calculate the average number of reviews per person\n",
        "avg_reviews_per_person = reviews_per_person.agg({\"count\": \"avg\"}).collect()[0][0]\n",
        "\n",
        "print(\"The average number of reviews per person is:\", avg_reviews_per_person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R9ozrxNPSNz",
        "outputId": "4cb88ee7-4a8f-477c-d9ce-0167570c435f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|count|\n",
            "+-----+\n",
            "|    6|\n",
            "|    5|\n",
            "|    4|\n",
            "|    4|\n",
            "|    4|\n",
            "|    4|\n",
            "|    3|\n",
            "|    3|\n",
            "|    3|\n",
            "|    3|\n",
            "+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#The average is very low so I check the first 10 customers by number of reviews.\n",
        "# 6 is the maximum number of reviews left by a user.  \n",
        "\n",
        "reviews_per_person.sort(desc(\"count\")).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idgF8oeZ8_kC",
        "outputId": "e1727794-e11d-4191-f358-1a09581f6332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+\n",
            "|product_id|       product_title|count|\n",
            "+----------+--------------------+-----+\n",
            "|B0051VVOB2|Kindle Fire (Prev...|  111|\n",
            "|B0083PWAPW|Kindle Fire HD 7\"...|   85|\n",
            "|B00JG8GOWU|Kindle Paperwhite...|   80|\n",
            "|B006GWO5WK|Amazon Kindle 9W ...|   77|\n",
            "|B00BWYQ9YE|Kindle Fire HDX 7...|   74|\n",
            "|B003L1ZYYM|AmazonBasics High...|   68|\n",
            "|B002Y27P3M|Kindle Keyboard, ...|   63|\n",
            "|B0015T963C|Kindle Wireless R...|   61|\n",
            "|B004XC6GJ0|ARRIS SURFboard D...|   57|\n",
            "|B00I15SB16|Kindle, 6\" Glare-...|   56|\n",
            "+----------+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#The first 10 most reviewed items\n",
        "\n",
        "sub_df.groupby('product_id', 'product_title').count().sort(desc(\"count\")).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ4UBvmVu9Lb",
        "outputId": "b7bac931-0ef5-45e3-ae3d-ddfca404f640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+-------------+-----------+\n",
            "|       product_title|star_rating|helpful_votes|customer_id|\n",
            "+--------------------+-----------+-------------+-----------+\n",
            "|Zune HD Video MP3...|          5|         2431|   52855449|\n",
            "|SanDisk Extreme P...|          4|         1920|   51960937|\n",
            "|Patagonia Kindle ...|          4|         1277|   12372811|\n",
            "|Samsung Chromeboo...|          5|         1109|   41866357|\n",
            "|Motorola SURFboar...|          4|          800|   28204599|\n",
            "|Samsung Galaxy Ta...|          4|          637|   45035381|\n",
            "|Datamancer The So...|          4|          618|   27055887|\n",
            "|Samsung Galaxy Ta...|          5|          515|   15208771|\n",
            "|Photive Hydra Wir...|          5|          437|   45843410|\n",
            "|ASUS MeMOPad HD 7...|          1|          424|   53090839|\n",
            "+--------------------+-----------+-------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# First 10 most useful reviews.\n",
        "\n",
        "sub_df.select(\"product_title\", \"star_rating\", \"helpful_votes\", \"customer_id\").sort(desc(\"helpful_votes\")).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9LMmz7sHp04m"
      },
      "outputs": [],
      "source": [
        "#First 10 days by total number of reviews\n",
        "\n",
        "# first cast the data type of \"review_date\" from string to date format\n",
        "\n",
        "from pyspark.sql.functions import to_date, lit\n",
        "sub_df = sub_df.withColumn(\"review_date\",col(\"review_date\").cast(DateType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NrxCmTbLC9u",
        "outputId": "244794c0-a1dc-45f8-b39f-e6490e396299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|review_date|count|\n",
            "+-----------+-----+\n",
            "| 2015-01-04|  125|\n",
            "| 2015-01-05|  105|\n",
            "| 2015-01-07|   99|\n",
            "| 2015-01-03|   97|\n",
            "| 2015-06-03|   91|\n",
            "| 2015-01-13|   91|\n",
            "| 2015-01-06|   88|\n",
            "| 2014-12-31|   84|\n",
            "| 2015-08-18|   82|\n",
            "| 2015-01-09|   82|\n",
            "+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#The highest numbers of reviews are left in January of years 2014 and 2015. \n",
        "#The possible explanation is that Christmas is the moment where most products are purchased\n",
        "#and after few days or weeks trying the product he/she leaves the reviews on Amazon. \n",
        "\n",
        "sub_df.groupby('review_date').count().sort(desc(\"count\")).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhZeVN6HWsP"
      },
      "source": [
        "<font size=\"5\">**4. PageRank Algorithm**</font> \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create the graph I want to compare two methods. The first one relies on the udf() a user defined function that allows to apply a function directly to a Pyspark dataframe. The second one relies only on for loops. "
      ],
      "metadata": {
        "id": "92nRsz5jaU-N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-GIHhuqS0Rf"
      },
      "source": [
        "**4.1 Creation of the Graph - Method with udf()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_ycZuF8K62M",
        "outputId": "58674302-b26d-4c13-940d-84ef93551eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|         customer_id|\n",
            "+----------+--------------------+\n",
            "|016642966X|          [19955871]|\n",
            "|0511189877|          [33355906]|\n",
            "|0972683275|[53074039, 435844...|\n",
            "|1394860919|          [16522736]|\n",
            "|1400501466|[51932300, 48648451]|\n",
            "|1400501776|          [49886899]|\n",
            "|1400532620|          [51280446]|\n",
            "|1400532655|          [17086455]|\n",
            "|140053271X|[25652547, 195277...|\n",
            "|1400599997|          [19325576]|\n",
            "+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#The first step is to create the edges of the graph. \n",
        "#An edge exists when two customers have reviewed the same product\n",
        "\n",
        "from pyspark.sql.functions import collect_set, sort_array, udf, explode\n",
        "from pyspark.sql.types import ArrayType, StringType, StructType, StructField\n",
        "\n",
        "# group by product_id and use collect_set() to aggregate the customer_id into an ArrayType to create a list of the customers by product\n",
        "custom_by_prod = sub_df.groupBy('product_id').agg(collect_set('customer_id').alias('customer_id'))\n",
        "custom_by_prod.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dYlWirBGUlvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84d34e8-a6d9-4939-ef62-dae8bae9b2dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 9.54 µs\n"
          ]
        }
      ],
      "source": [
        "#METHOD WITH UDF \n",
        "%%time\n",
        "\n",
        "# The function create_edges() creates an edge between two customers who reviewed the same product\n",
        "# by creating a tuple containing the customer ID of the two people. \n",
        "def create_edges(customer_id):\n",
        "    edges = []\n",
        "    for i in range(len(customer_id)):\n",
        "        for j in range(i+1, len(customer_id)):\n",
        "            edge = tuple(sorted([customer_id[i], customer_id[j]]))\n",
        "            edges.append(edge)\n",
        "    return edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaDeeYO5YnNq",
        "outputId": "c91a80d5-1a3e-4302-e59a-17c3bd8d6cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.7 ms, sys: 2.24 ms, total: 15.9 ms\n",
            "Wall time: 237 ms\n"
          ]
        }
      ],
      "source": [
        "# edge_udf() allows to apply the create_edges() function to a PySpark DataFrame. \n",
        "# the UDF will return an array of tuples, each of which has two string fields.\n",
        "# However udf() is very expensive\n",
        "\n",
        "%%time\n",
        "\n",
        "edge_udf = udf(create_edges, ArrayType(StructType([\n",
        "    StructField('src', StringType()),\n",
        "    StructField('dst', StringType())])))\n",
        "\n",
        "# edges is a new DataFrame with a row for each edge between customers that rated the same product ID\n",
        "\n",
        "df_edges = custom_by_prod.select('customer_id').withColumn('edges', edge_udf(sort_array('customer_id'))).select(explode('edges').alias('edge'))\n",
        "#df_edges.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1agwqjv8hTO",
        "outputId": "8c7eb146-f220-4864-90e5-47264bb06677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'016642966X': ['19955871'], '0511189877': ['33355906'], '0972683275': ['53074039', '43584487', '29997920', '41490022', '26324308', '26780671', '13630979', '11988204'], '1394860919': ['16522736'], '1400501466': ['51932300', '48648451']}\n"
          ]
        }
      ],
      "source": [
        "#JUST TO TEST \n",
        "\n",
        "#In order to verify that the edges have been created correctly I create a dictionary from custom_by_prod \n",
        "#and display the first 5 elements. I choose customer_id '53074039' because it is the first one \n",
        "# with edges and I retrieve the same customer_id from edges dataframe. If the edges in common coincide the \n",
        "#dataframe edges has been created correctly \n",
        "\n",
        "# convert the DataFrame to a list of Row objects and create a dictionary\n",
        "custom_dict = {row['product_id']: row['customer_id'] for row in custom_by_prod.collect()}\n",
        "\n",
        "# print the first 10 entries in the dictionary\n",
        "print(dict(list(custom_dict.items())[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9cn2Fzo_3oQ",
        "outputId": "d6022a24-ff7e-4a7a-b497-82c2b443c20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                edge|\n",
            "+--------------------+\n",
            "|[11988204, 53074039]|\n",
            "|[13630979, 53074039]|\n",
            "|[26324308, 53074039]|\n",
            "|[26780671, 53074039]|\n",
            "|[29997920, 53074039]|\n",
            "|[41490022, 53074039]|\n",
            "|[43584487, 53074039]|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#The edges linked to customer 53074039 coincide both in the dataframe and in the dictionary. \n",
        "# Therefore we can go on with the analysis. \n",
        "\n",
        "df_edges.filter(df_edges.edge.dst == '53074039' ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDM7Dn0cGE34",
        "outputId": "509ebd34-b6d2-4b22-dcb7-1bfabc534f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.09 s, sys: 282 ms, total: 3.37 s\n",
            "Wall time: 3min 15s\n"
          ]
        }
      ],
      "source": [
        "#create the network \n",
        "%%time\n",
        "\n",
        "#create lists from customer_id and edges to iterate on them \n",
        "\n",
        "custom_list = list(sub_df.select('customer_id').toPandas()['customer_id'])\n",
        "\n",
        "edges_list = list(df_edges.select('edge').toPandas()['edge'])\n",
        "#print(edges_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's substitute the nodes with some indexes to save memory and use them later in the PageRank implementation \n",
        "# Get list of unique nodes\n",
        "n_int_list = list(set([e[0] for e in edges_list] + [e[1] for e in edges_list]))\n",
        "n_int_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f5db8c-7ffd-4998-cb1f-eeab779ee4d6",
        "id": "8VdTbcowqyqn"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['36506801', '14234944', '42926371', '44108825', '52980765']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUSY0EPrIyyI",
        "outputId": "ddd5cef6-add6-4d71-e04e-d9fce80d31a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(range(0, 26968), range(0, 118183))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "nodes = range(len(n_int_list))\n",
        "links = range(len(edges_list))\n",
        "nodes, links"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary mapping nodes to a corresponding index\n",
        "\n",
        "%%time\n",
        "n_dict = {node: index for index, node in enumerate(n_int_list)}\n",
        "\n",
        "# Replace node names with their indices in the edge list\n",
        "links_idx_list = [(n_dict[a], n_dict[b]) for (a, b) in edges_list]\n",
        "print(links_idx_list[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd806dc8-88ea-4f94-8ab9-c370a98d6626",
        "id": "ulrRv5-RqyrZ"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(142, 12808), (142, 2245), (142, 15319), (142, 26881), (142, 16740)]\n",
            "CPU times: user 59.3 ms, sys: 2.5 ms, total: 61.8 ms\n",
            "Wall time: 61.9 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0PE_xZBnMCoG"
      },
      "outputs": [],
      "source": [
        "#create a dictionaty with a numeric label for each node and each edge\n",
        "\n",
        "node_id = {c: i for i, c in enumerate(n_int_list)}\n",
        "links_id = {c: i for i, c in enumerate(edges_list)}\n",
        "#print(node_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o4BBH_tIJjO",
        "outputId": "33f6def2-7d49-429e-f487-e3382443e20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 26968\n",
            "Number of edges: 118183\n",
            "Is the graph connected? False\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.Graph()\n",
        " \n",
        "for n in nodes:\n",
        "    node = G.add_node(n)\n",
        " \n",
        "for (a,b) in links_id:\n",
        "    G.add_edge(node_id[a], node_id[b])\n",
        "\n",
        "print('Number of nodes:', G.number_of_nodes())\n",
        "print('Number of edges:', G.number_of_edges())\n",
        "print('Is the graph connected?', nx.is_connected(G))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Creation of the Graph - Method with for loops**"
      ],
      "metadata": {
        "id": "Drzrz_QjbFvZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVmwEnz8canh",
        "outputId": "ee166ed3-2a10-4d76-a5b7-7acd5d6b7c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.97 s, sys: 37.3 ms, total: 2.01 s\n",
            "Wall time: 2.62 s\n"
          ]
        }
      ],
      "source": [
        "#METHOD WITH FOR LOOPS \n",
        "%%time\n",
        "from pyspark.sql.functions import collect_set\n",
        "from itertools import combinations\n",
        "\n",
        "# create an empty list to store the edges\n",
        "edges = []\n",
        "\n",
        "# iterate through each row of custom_by_prod dataframe\n",
        "for row in custom_by_prod.collect():\n",
        "    customer_ids = row.customer_id\n",
        "    # generate all possible combinations of the customer_ids for each product\n",
        "    customer_pairs = combinations(customer_ids, 2)\n",
        "    # append each customer pair as an edge to the edges list\n",
        "    for pair in customer_pairs:\n",
        "        edges.append(pair)\n",
        "\n",
        "# convert the edges list to a PySpark dataframe\n",
        "edges_df = spark.createDataFrame(edges, ['src', 'dst'])\n",
        "#edges_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSPz_YNxde-6",
        "outputId": "37c75f3f-34fe-4c59-8c19-9f1fd7494f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('53074039', '43584487'), ('53074039', '29997920'), ('53074039', '41490022'), ('53074039', '26324308'), ('53074039', '26780671')]\n",
            "CPU times: user 931 ms, sys: 33.5 ms, total: 965 ms\n",
            "Wall time: 2.29 s\n"
          ]
        }
      ],
      "source": [
        "# edges_df.collect() returns a list of Row objects, where each Row represents a row in the pyspark dataframe.\n",
        "# Then the for loop iterates on each Row and returns a list of tuples\n",
        "\n",
        "%%time\n",
        "ed_list = [tuple(row) for row in edges_df.collect()]\n",
        "print(ed_list[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's substitute the nodes with some indexes to save memory and use them later in the PageRank implementation \n",
        "# Get list of unique nodes\n",
        "nodes_list = list(set([e[0] for e in ed_list] + [e[1] for e in ed_list]))\n",
        "nodes_list[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEMFeh9ml2tA",
        "outputId": "f79ff351-c2ab-45da-ac10-075e30f27964"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['36506801', '14234944', '42926371', '44108825', '52980765']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary mapping nodes to a corresponding index\n",
        "\n",
        "%%time\n",
        "node_dict = {node: index for index, node in enumerate(nodes_list)}\n",
        "\n",
        "# Replace node names with their indices in the edge list\n",
        "ed_idx_list = [(node_dict[a], node_dict[b]) for (a, b) in ed_list]\n",
        "#print(ed_idx_list)"
      ],
      "metadata": {
        "id": "h8loNY8Wm7d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4809b95f-85c4-4ae1-ddcb-a66d1d23f6ae"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 61 ms, sys: 0 ns, total: 61 ms\n",
            "Wall time: 61.1 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "v8h2H3iZg7uF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0276d60e-92f1-4cc9-8bce-7ec124c79aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 26968\n",
            "Number of edges: 118183\n",
            "Is the graph connected? False\n",
            "CPU times: user 405 ms, sys: 121 ms, total: 526 ms\n",
            "Wall time: 552 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "import networkx as nx\n",
        "graph = nx.Graph()\n",
        "\n",
        "num_of_nodes = range(len(nodes_list))\n",
        " \n",
        "for n in num_of_nodes:\n",
        "    node = graph.add_node(n)\n",
        " \n",
        "for (a,b) in ed_idx_list:\n",
        "    graph.add_edge(num_of_nodes[a], num_of_nodes[b])\n",
        "\n",
        "print('Number of nodes:', graph.number_of_nodes())\n",
        "print('Number of edges:', graph.number_of_edges())\n",
        "print('Is the graph connected?', nx.is_connected(graph))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On average each node has around 4.38 edges."
      ],
      "metadata": {
        "id": "oUIsJWcJ5GgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The udf() is a lot faster than for loops. If I were to compare the two methods on the entire dataset the cost of using for loops would be definitely higher.  "
      ],
      "metadata": {
        "id": "d0Dbc_FBp_06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The top 3 node with the most links  \n",
        "\n",
        "node_degrees = list(graph.degree())\n",
        "node_degrees.sort(key=lambda x: x[1], reverse=True)\n",
        "top_nodes = [node_degrees[i][0] for i in range(3)]\n",
        "print(dict(graph.degree(nbunch = top_nodes)).items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUbqZ3GUrIAE",
        "outputId": "8e0406c2-19b0-4873-c78e-acbcb293b13e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(6028, 160), (8868, 119), (15959, 113)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbiw2laod4e5"
      },
      "source": [
        "**4.2 Creation of the Transition Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_xdQ8MevSry",
        "outputId": "7c70e4d4-87f6-40bc-ccc8-6fadf2f5aedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(9023, 14880), (9023, 26881), (9023, 16740), (9023, 2245), (9023, 15319)]\n"
          ]
        }
      ],
      "source": [
        "#cast the type from string to int\n",
        "\n",
        "ed_int_idx_list = [(int(x), int(y)) for (x, y) in ed_idx_list]\n",
        "print(ed_int_idx_list[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "48L5EPQahBTF"
      },
      "outputs": [],
      "source": [
        "# Create an empty dictionary with nodes as keys and an empty list as values\n",
        "adjacency = {}\n",
        "\n",
        "# The set() function is used to create a set of all unique nodes in the graph. \n",
        "# The adjacency dictionary is created by adding all the source and destination nodes of each edge to the set.\n",
        "\n",
        "for n in set([e[0] for e in ed_int_idx_list] + [e[1] for e in ed_int_idx_list]):\n",
        "    adjacency[n] = []\n",
        "#print(adjacency)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH0H_j_LyFR-",
        "outputId": "8598805e-4711-4fbd-e1a6-db413a074f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [18240, 7648], 1: [22247, 6701, 1480, 19196, 24579, 13216, 13317, 11040, 11150, 14857, 23526, 25420, 14851, 10205, 14377, 20358, 7158, 25777, 3794, 19026, 4925, 22371, 7949, 6513, 5313], 2: [23852], 3: [7169], 4: [24536, 18697, 13684, 21972]}\n"
          ]
        }
      ],
      "source": [
        "# Iterate over the edges and add the target node to the adjacency list of the source node\n",
        "\n",
        "for (n1, n2) in ed_int_idx_list:\n",
        "    adjacency[n1].append(n2)\n",
        "    adjacency[n2].append(n1)  # as the graph is undirected add the reverse edge as well\n",
        "\n",
        "# Print the adjacency dictionary\n",
        "print(dict(list(adjacency.items())[0:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARnLR8x76PwQ",
        "outputId": "6083010b-783d-4f1f-9183-3195f31e0cbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(18240, 0, 0.5),\n",
              " (7648, 0, 0.5),\n",
              " (22247, 1, 0.04),\n",
              " (6701, 1, 0.04),\n",
              " (1480, 1, 0.04),\n",
              " (19196, 1, 0.04),\n",
              " (24579, 1, 0.04),\n",
              " (13216, 1, 0.04),\n",
              " (13317, 1, 0.04),\n",
              " (11040, 1, 0.04)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "#an empty list is created to store the transition probabilities.\n",
        "\n",
        "transition_matrix = []\n",
        "for n1 in adjacency:\n",
        "    for n2 in adjacency[n1]:\n",
        "        # For each pair (n1,n2) of adjacent nodes, a tuple with 3 values is appended.\n",
        "        #The first value n2 is the destination node, the second the source node, \n",
        "        #and the third value is the probability of moving from node n1 to node n2\n",
        "        transition_matrix.append((n2, n1, 1./len(adjacency[n1])))\n",
        "transition_matrix[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Parallelization**"
      ],
      "metadata": {
        "id": "u1szZWl7wS2w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiGDr3v2cikn",
        "outputId": "3aa24430-ddff-433e-e02d-c6f3ee8657b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 7648, 0.5),\n",
              " (0, 18240, 0.5),\n",
              " (1, 1480, 0.04),\n",
              " (1, 3794, 0.041666666666666664),\n",
              " (1, 4925, 0.041666666666666664),\n",
              " (1, 5313, 1.0),\n",
              " (1, 6513, 0.041666666666666664),\n",
              " (1, 6701, 0.041666666666666664),\n",
              " (1, 7158, 0.041666666666666664),\n",
              " (1, 7949, 0.041666666666666664)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Caching the result of the transformation is one of the optimization tricks to\n",
        "# improve the performance of the long-running PySpark applications/jobs.\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sparkContext=spark.sparkContext\n",
        "edges_rdd = sparkContext.parallelize(transition_matrix).cache()\n",
        "edges_rdd.sortByKey().take(10)\n",
        "\n",
        "#the result is a list of triples of the form (i, j, m_ij)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The initial vector represents the probability distribution of a random surfer starting in \n",
        "#any of the customer nodes. \n",
        "\n",
        "import numpy as np\n",
        "n = len(num_of_nodes)\n",
        "page_rank = np.ones(n)/n\n",
        "\n",
        "# Set convergence threshold\n",
        "threshold = 0.0001\n",
        "max_rep = 500\n",
        "# Initialize iteration count and residual error\n",
        "iteration = 0\n",
        "residual_error = float('inf')\n",
        "\n",
        "# Run iterations until convergence\n",
        "while residual_error > threshold and iteration < max_rep:\n",
        "    # Calculate new page rank values \n",
        "    new_page_rank_values = edges_rdd.map(lambda i_j_mij: (i_j_mij[0], i_j_mij[2]*page_rank[i_j_mij[1]]))\n",
        "                     \n",
        "    new_page_rank_values = new_page_rank_values.reduceByKey(lambda a, b: a+b).collect()\n",
        "\n",
        "    # Calculate residual error\n",
        "    residual_error = sum(abs(c - page_rank[i]) for (i, c) in new_page_rank_values)\n",
        "\n",
        "    page_rank = np.array([c for (i, c) in new_page_rank_values])\n",
        "\n",
        "    #increment iteration count\n",
        "    iteration += 1\n",
        "\n",
        "    # Print final results\n",
        "print(\"Converged after\", iteration, \"iterations\")\n",
        "print(page_rank[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CZ0QL2K3s60",
        "outputId": "2e3ef67b-ccef-4c6d-cb02-8c8a64cdf73b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 500 iterations\n",
            "[3.70666197e-05 3.69146024e-05 3.48185448e-05 3.48631896e-05\n",
            " 7.14783159e-05 3.48422312e-05 3.48083591e-05 3.48426897e-05\n",
            " 3.47974385e-05 3.47758226e-05 3.48051743e-05 3.51569230e-05\n",
            " 3.61238147e-05 3.48116515e-05 3.47916589e-05 3.48073982e-05\n",
            " 3.48198802e-05 3.47982801e-05 3.48128496e-05 3.48773769e-05\n",
            " 3.48190426e-05 3.50010041e-05 3.47689842e-05 3.48139688e-05\n",
            " 3.48149974e-05 3.48079324e-05 1.47658409e-06 3.48185448e-05\n",
            " 3.48631896e-05 4.57807281e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_cQb2dYvQ5T"
      },
      "source": [
        "**4.4 Teleport Variation**\n",
        "\n",
        "The implementation of PageRank in this way it is effective only if the graph is strongly connected and doesn't present dead ends nor spider traps. \n",
        "In the graph obtained dead ends are not possible because each edge is bidirectional, however it is not connected. This means that there could be spider traps. Spider traps are a set of nodes with no dead ends, but no arcs\n",
        "out. To avoid this problem the calculation of PageRank is modified by allowing each random surfer a small probability beta to be teleported to a random page. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_rdd = sparkContext.parallelize(transition_matrix).cache()\n",
        "\n",
        "n = len(num_of_nodes)\n",
        "page_rank_t = np.ones(n)/n\n",
        "\n",
        "# Set convergence threshold\n",
        "threshold = 0.0001\n",
        "max_rep = 500\n",
        "# Initialize iteration count and residual error\n",
        "iteration = 0\n",
        "residual_error = float('inf')\n",
        "\n",
        "# The conventional value of beta is 0.85\n",
        "beta = 0.85\n",
        "\n",
        "# Run iterations until convergence\n",
        "while residual_error > threshold and iteration < max_rep:\n",
        "    # Calculate new page rank values \n",
        "    new_page_rank_values_t = edges_rdd.map(lambda i_j_mij: (i_j_mij[0], i_j_mij[2]*page_rank_t[i_j_mij[1]]))\n",
        "                     \n",
        "    new_page_rank_values_t = new_page_rank_values_t.reduceByKey(lambda a, b: a+b).collect()\n",
        "\n",
        "    # Calculate residual error\n",
        "    residual_error = sum(abs(c - page_rank_t[i]) for (i, c) in new_page_rank_values_t)\n",
        "\n",
        "    page_rank_t = np.array([beta * c + (1 - beta) * 1.0/n for (i, c) in new_page_rank_values_t])\n",
        "\n",
        "    #increment iteration count\n",
        "    iteration += 1\n",
        "\n",
        "    # Print final results\n",
        "print(\"Converged after\", iteration, \"iterations\")\n",
        "print(page_rank_t[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVAARg1MZBMf",
        "outputId": "e72641db-e9f8-4260-f511-a77f70c5fef8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 500 iterations\n",
            "[3.70006287e-05 3.69808822e-05 3.56331992e-05 3.56556137e-05\n",
            " 6.67885463e-05 3.56445968e-05 3.56311188e-05 3.56451396e-05\n",
            " 3.56261960e-05 3.56210164e-05 3.56304201e-05 3.57857788e-05\n",
            " 3.65848866e-05 3.56313323e-05 3.56266330e-05 3.56349756e-05\n",
            " 3.56340369e-05 3.56286290e-05 3.56353522e-05 3.56595108e-05\n",
            " 3.56409313e-05 3.57301101e-05 3.56180653e-05 3.56350141e-05\n",
            " 3.56363551e-05 3.56324800e-05 6.81949773e-06 3.58503670e-05\n",
            " 3.58694194e-05 4.34013736e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the difference in the scores between PageRank scores and its variation \n",
        "\n",
        "diff_pagerank = page_rank - page_rank_t\n",
        "diff_pagerank[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKJswPd6iI9q",
        "outputId": "74e17ac0-68bc-41c4-c2f6-c6fa71471720"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6.59910179e-08, -6.62798551e-08, -8.14654405e-07, -7.92424162e-07,\n",
              "        4.68976959e-06, -8.02365691e-07, -8.22759681e-07, -8.02449838e-07,\n",
              "       -8.28757453e-07, -8.45193778e-07, -8.25245805e-07, -6.28855767e-07,\n",
              "       -4.61071979e-07, -8.19680809e-07, -8.34974121e-07, -8.27577383e-07,\n",
              "       -8.14156686e-07, -8.30348876e-07, -8.22502542e-07, -7.82133952e-07,\n",
              "       -8.21888699e-07, -7.29105997e-07, -8.49081151e-07, -8.21045224e-07,\n",
              "       -8.21357684e-07, -8.24547592e-07, -5.34291363e-06, -1.03182226e-06,\n",
              "       -1.00622983e-06,  2.37935450e-06,  2.37943382e-06,  2.43936799e-06,\n",
              "        2.32181926e-06, -1.67725546e-07, -9.98994484e-08, -7.97077152e-08,\n",
              "       -8.38395567e-09, -6.26234132e-08, -1.04023959e-06,  1.19267450e-06,\n",
              "        7.03899969e-07,  1.11774289e-06,  1.22010902e-06,  1.21560184e-06,\n",
              "        1.05448713e-06,  1.20162793e-06,  1.20412156e-06,  1.19912964e-06,\n",
              "        1.15974806e-06,  1.19774051e-06])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store the PageRank with teleport scores for each node\n",
        "pagerank_scores = {}\n",
        "for i, node in enumerate(node_dict.keys()):\n",
        "    pagerank_scores[node] = page_rank[i]\n",
        "\n",
        "# Print the top 10 nodes with the highest PageRank scores\n",
        "top_nodes = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "for node, score in top_nodes:\n",
        "    print(\"Node {}: PageRank score = {:.5f}\".format(node, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V2jFDTnOs-0",
        "outputId": "b945e931-a3ce-46b3-d6c8-bdae61517189"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 39851134: PageRank score = 0.00014\n",
            "Node 14257719: PageRank score = 0.00011\n",
            "Node 37315844: PageRank score = 0.00011\n",
            "Node 48784470: PageRank score = 0.00011\n",
            "Node 16276436: PageRank score = 0.00011\n",
            "Node 9913444: PageRank score = 0.00011\n",
            "Node 10372192: PageRank score = 0.00011\n",
            "Node 22249337: PageRank score = 0.00009\n",
            "Node 13331217: PageRank score = 0.00009\n",
            "Node 12406773: PageRank score = 0.00008\n",
            "Node 52974830: PageRank score = 0.00008\n",
            "Node 21433522: PageRank score = 0.00008\n",
            "Node 52363526: PageRank score = 0.00008\n",
            "Node 16741753: PageRank score = 0.00008\n",
            "Node 5448082: PageRank score = 0.00008\n",
            "Node 44848235: PageRank score = 0.00008\n",
            "Node 42277193: PageRank score = 0.00008\n",
            "Node 1404310: PageRank score = 0.00008\n",
            "Node 16455264: PageRank score = 0.00008\n",
            "Node 25077527: PageRank score = 0.00008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store the PageRank with teleport scores for each node\n",
        "pagerank_t_scores = {}\n",
        "for i, node in enumerate(node_dict.keys()):\n",
        "    pagerank_t_scores[node] = page_rank_t[i]\n",
        "\n",
        "# Print the top 10 nodes with the highest PageRank scores\n",
        "top_nodes_t = sorted(pagerank_t_scores.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "for node, score in top_nodes_t:\n",
        "    print(\"Node {}: PageRank score = {:.5f}\".format(node, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgkejPPRpve5",
        "outputId": "b244853a-136d-4268-be7a-7664defc4ad8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 39851134: PageRank score = 0.00013\n",
            "Node 48784470: PageRank score = 0.00010\n",
            "Node 14257719: PageRank score = 0.00010\n",
            "Node 10372192: PageRank score = 0.00010\n",
            "Node 16276436: PageRank score = 0.00009\n",
            "Node 37315844: PageRank score = 0.00009\n",
            "Node 9913444: PageRank score = 0.00009\n",
            "Node 22249337: PageRank score = 0.00008\n",
            "Node 13331217: PageRank score = 0.00008\n",
            "Node 12406773: PageRank score = 0.00008\n",
            "Node 21433522: PageRank score = 0.00007\n",
            "Node 52974830: PageRank score = 0.00007\n",
            "Node 5448082: PageRank score = 0.00007\n",
            "Node 52363526: PageRank score = 0.00007\n",
            "Node 1404310: PageRank score = 0.00007\n",
            "Node 50339489: PageRank score = 0.00007\n",
            "Node 16741753: PageRank score = 0.00007\n",
            "Node 42277193: PageRank score = 0.00007\n",
            "Node 2535508: PageRank score = 0.00007\n",
            "Node 22539698: PageRank score = 0.00007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font size=\"5\">5. Introduction of the \"helpfulness vector\"</font>"
      ],
      "metadata": {
        "id": "gyCTF62ostlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pagerank values are all pretty low which is normal as in the exploratory analysis it was already shown that the most of the customers had few reviews in common. This is also because the sample chosen is very restricted compared to the size of the original dataset. \n",
        "Furthermore, many nodes have exactly the same score. For this reason I decide to introduce the variable \"helpful_votes\", to try to diversify between who left a useful review and who didn't, in order to create a better ordering. "
      ],
      "metadata": {
        "id": "sLuUVgSsq027"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by customer_id and collect product_ids into a list\n",
        "hv_df = sub_df.select(\"customer_id\", \"helpful_votes\").groupBy(\"customer_id\").sum(\"helpful_votes\").alias(\"helpful_votes\")\n",
        "hv_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uD5epBQTBKt",
        "outputId": "29376b51-c831-4409-f41b-380de90b038d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|customer_id|sum(helpful_votes)|\n",
            "+-----------+------------------+\n",
            "|   42706553|                 1|\n",
            "|   19159455|                 0|\n",
            "|   52808364|                 0|\n",
            "|   12774562|                 6|\n",
            "|   22295949|                 0|\n",
            "|   47440766|                11|\n",
            "|   12468782|                 0|\n",
            "|   18658791|                 0|\n",
            "|   22500595|                 0|\n",
            "|   14340978|                 0|\n",
            "+-----------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find the total number of times the reviews have been considered useful \n",
        "\n",
        "filtered_hv_df = hv_df.filter(col(\"customer_id\").isin(nodes_list))\n",
        "total_hv = filtered_hv_df.groupBy().sum(\"sum(helpful_votes)\").collect()[0][0]\n",
        "total_hv"
      ],
      "metadata": {
        "id": "4__7a3twUvbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa5357c-a978-4b7c-9b31-f72822c8c594"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42957"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the helpfulness score for each customer  \n",
        "\n",
        "filtered_hv_df = filtered_hv_df.withColumn('helpful_votes', col('sum(helpful_votes)') / total_hv)\n",
        "filtered_hv_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9eF_YeOYxAc",
        "outputId": "942b2ed5-5b84-4914-b9df-1ca8369fe95f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+--------------------+\n",
            "|customer_id|sum(helpful_votes)|       helpful_votes|\n",
            "+-----------+------------------+--------------------+\n",
            "|   52808364|                 0|                 0.0|\n",
            "|   22295949|                 0|                 0.0|\n",
            "|   47440766|                11|2.560700235118839...|\n",
            "|   12468782|                 0|                 0.0|\n",
            "|   14340978|                 0|                 0.0|\n",
            "|   39528786|                 0|                 0.0|\n",
            "|   23382296|                 1|2.327909304653490...|\n",
            "|    7801327|                 0|                 0.0|\n",
            "|   24225794|                 0|                 0.0|\n",
            "|    2862763|                 0|                 0.0|\n",
            "+-----------+------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dictinary with as key the customer_id and value the \"helpfulness\"\n",
        "hv_dict = dict(filtered_hv_df.rdd.map(lambda x: (x[0], x[2])).collect())\n"
      ],
      "metadata": {
        "id": "bQWgPNzTbBr3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pagerank scores\n",
        "\n",
        "pagerank_t_scores = {}\n",
        "for i, node in enumerate(node_dict.keys()):\n",
        "    pagerank_t_scores[node] = page_rank_t[i]"
      ],
      "metadata": {
        "id": "U4UK_DhPA8Ko"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum the helpfulness score to the pagerank score for each customer \n",
        "\n",
        "final_score = {k: pagerank_t_scores.get(k, 0) + hv_dict.get(k, 0) for k in set(pagerank_t_scores) | set(hv_dict)}\n",
        "#final_score"
      ],
      "metadata": {
        "id": "wyQRnxwMPeLv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 10 nodes with the highest final score \n",
        "\n",
        "top_nodes_t = sorted(final_score.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "for node, score in top_nodes_t:\n",
        "    print(\"Node {}: Final score = {:.5f}\".format(node, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GONZHOpLftuU",
        "outputId": "d1d1aa60-fdb6-4206-c742-0ea87ed29341"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 52855449: Final score = 0.05663\n",
            "Node 12372811: Final score = 0.02976\n",
            "Node 41866357: Final score = 0.02586\n",
            "Node 45035381: Final score = 0.01486\n",
            "Node 15208771: Final score = 0.01203\n",
            "Node 53090839: Final score = 0.01128\n",
            "Node 45843410: Final score = 0.01021\n",
            "Node 51840028: Final score = 0.00953\n",
            "Node 50695896: Final score = 0.00951\n",
            "Node 18833993: Final score = 0.00921\n",
            "Node 30669680: Final score = 0.00898\n",
            "Node 35832624: Final score = 0.00865\n",
            "Node 41766042: Final score = 0.00798\n",
            "Node 31864628: Final score = 0.00779\n",
            "Node 52860694: Final score = 0.00737\n",
            "Node 32684861: Final score = 0.00709\n",
            "Node 52810431: Final score = 0.00681\n",
            "Node 51297632: Final score = 0.00600\n",
            "Node 35360153: Final score = 0.00595\n",
            "Node 51721371: Final score = 0.00567\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyZaIR/klZKg7pKat4sv99",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}